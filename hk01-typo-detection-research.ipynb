{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15280,"status":"ok","timestamp":1703309542337,"user":{"displayName":"Andy Ching","userId":"04259283919464139952"},"user_tz":-480},"id":"Yja0IANEJ-qP","outputId":"07a6f80c-ba0a-4584-afd7-7024f3f86bb5"},"outputs":[{"name":"stderr","output_type":"stream","text":["UsageError: Line magic function `%pip3` not found.\n"]}],"source":["%pip install tensorflow_text tensorflow_hub"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":9574,"status":"ok","timestamp":1703309566203,"user":{"displayName":"Andy Ching","userId":"04259283919464139952"},"user_tz":-480},"id":"DvXu-ZDImDJX"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_text as text\n","import tensorflow_hub as hub\n","import pandas as pd\n","import numpy as np\n","from functools import cache\n","import re"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":307,"status":"ok","timestamp":1703309571089,"user":{"displayName":"Andy Ching","userId":"04259283919464139952"},"user_tz":-480},"id":"blpUwVOGmDJZ"},"outputs":[],"source":["class TypoDetector:\n","\n","    def __init__(\n","            self,\n","            preprocessor_handle=\"https://tfhub.dev/tensorflow/bert_zh_preprocess/3\",\n","            encoder_handle=\"https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12/4\",\n","            seq_length: int = 128,\n","            trainable: bool = True) -> None:\n","        self.preprocessor_handle = preprocessor_handle\n","        self.encoder_handle = encoder_handle\n","        self.seq_length = seq_length\n","        self.trainable = trainable\n","        self.preprocessor = hub.load(preprocessor_handle)\n","        self.encoder = hub.load(encoder_handle)\n","\n","    @cache\n","    def get_vocab(self) -> list[str]:\n","        \"\"\"\n","        Get the vocab dictionary from encoder\n","        :returns: the vocab list\n","        \"\"\"\n","        vocab_filepath = self.encoder.vocab_file.asset_path.numpy().decode(\n","            \"utf-8\")\n","        with open(vocab_filepath, 'r') as f:\n","            return [vocab[:-1] for vocab in f]\n","\n","    @cache\n","    def get_vocab_size(self) -> int:\n","        \"\"\"\n","        Get the length of the vocab list\n","        :returns: vocab size\n","        \"\"\"\n","        return self.preprocessor.tokenize.get_special_tokens_dict(\n","        )['vocab_size']\n","\n","    def get_tokenizer(self) -> hub.KerasLayer:\n","        \"\"\"\n","        Get the tokenizer as the preprocessor of the bert model\n","        :returns: the tokenizer\n","        \"\"\"\n","        return hub.KerasLayer(self.preprocessor.tokenize)\n","\n","    def get_model(self) -> tf.keras.Model:\n","        \"\"\"\n","        Compose and return our detector model\n","        :returns: the detector model\n","        \"\"\"\n","        # input layer\n","        text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n","        # tokenizer layer\n","        tokenizer = self.get_tokenizer()\n","        # pack-inputs layer\n","        bert_pack_inputs = hub.KerasLayer(\n","            self.preprocessor.bert_pack_inputs,\n","            arguments={\"seq_length\": tf.constant(self.seq_length)})\n","        # mlm layer\n","        mlm_layer = hub.KerasLayer(self.encoder.mlm, trainable=self.trainable)\n","        # string input to tokens\n","        tokenized_input = tokenizer(text_input)\n","        # tokens to bert encoder inputs\n","        encoder_inputs = bert_pack_inputs([tokenized_input])\n","        # bert encoder inputs to mlm inputs\n","        mlm_inputs = self.__to_mlm_inputs(encoder_inputs)\n","        # consume mlm inputs and produce corresponding outputs by the mlm model\n","        mlm_outputs = mlm_layer(mlm_inputs)\n","        # convert mlm outputs to probabilities of each char of the input\n","        logit_probs = self.__to_logit_probs(mlm_outputs)\n","        char_prob = self.__to_char_prob(encoder_inputs, logit_probs)\n","        top1_prob, top1_token = self.__to_topk_probs(logit_probs, 1)\n","        top1_prob = tf.squeeze(top1_prob, axis=-1)\n","        top1_token = tf.squeeze(top1_token, axis=-1)\n","        char_token = encoder_inputs['input_word_ids']\n","        outputs = {\n","            'char_token': char_token,\n","            'char_prob': char_prob,\n","            'top1_token': top1_token,\n","            'top1_prob': top1_prob,\n","        }\n","        # pack the layers as tf model\n","        return tf.keras.Model(text_input, outputs)\n","\n","    def __to_logit_probs(self, mlm_outputs: dict) -> tf.Tensor:\n","        \"\"\"\n","        Convert mlm outputs to the probabilities of each char in vocab\n","        :returns: shape=(batch_size, seq_length, vocab_size)\n","        \"\"\"\n","        return tf.keras.layers.Softmax()(mlm_outputs[\"mlm_logits\"])\n","\n","    def __to_topk_probs(self, logit_probs, k: int = 1) -> list[tf.Tensor]:\n","        \"\"\"\n","        Convert mlm outputs to the indices of top K chars\n","        :param logit_probs: returns of self.__to_logit_probs()\n","        :param k: number of items at the top most to take, optional\n","        :returns: list(values=[shape=(batch_size, seq_length, k)], indices=[shape=(batch_size, seq_length, k)])\n","        \"\"\"\n","        return tf.math.top_k(logit_probs, k)\n","\n","    def __to_char_prob(self, encoder_inputs: dict, logit_probs) -> tf.Tensor:\n","        \"\"\"\n","        Convert the mlm outputs to the probabilities of each char of sentences\n","        :param encoder_inputs: inputs for the bert model\n","        :param logit_probs: returns of self.__to_logit_probs()\n","        :returns: shape=(batch_size, seq_length)\n","        \"\"\"\n","        vocab_size = self.get_vocab_size()\n","        one_hot_token_ids = tf.one_hot(encoder_inputs[\"input_word_ids\"],\n","                                       vocab_size)\n","        return tf.reduce_max(tf.multiply(logit_probs, one_hot_token_ids), -1)\n","\n","    def __to_mlm_inputs(self, encoder_inputs: dict) -> dict[str, tf.Tensor]:\n","        \"\"\"\n","        Convert bert encoder inputs input mlm inputs\n","        :returns: the corresponding inputs for bert's mlm model\n","        \"\"\"\n","        masked_lm_positions = tf.multiply(\n","            tf.ones_like(encoder_inputs[\"input_word_ids\"], dtype=tf.int32),\n","            tf.transpose(tf.range(0, self.seq_length)))\n","        return {\n","            \"input_word_ids\": encoder_inputs[\"input_word_ids\"],\n","            \"input_mask\": encoder_inputs[\"input_mask\"],\n","            \"input_type_ids\": encoder_inputs[\"input_type_ids\"],\n","            \"masked_lm_positions\": masked_lm_positions,\n","        }\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":24926,"status":"ok","timestamp":1703309600794,"user":{"displayName":"Andy Ching","userId":"04259283919464139952"},"user_tz":-480},"id":"Y1q4XZE0mDJa"},"outputs":[],"source":["detector = TypoDetector()\n","model = detector.get_model()\n","vocab = detector.get_vocab()"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":287,"status":"ok","timestamp":1703309603536,"user":{"displayName":"Andy Ching","userId":"04259283919464139952"},"user_tz":-480},"id":"ToH3X9BimDJa"},"outputs":[],"source":["class SentenceInfoPraser:\n","    \"\"\"\n","    Example:\n","    sentenceInfoParser = SentenceInfoPraser(vocab)\n","    sentenceInfoParser.parse_sentences([\"「Star War」很好看。\"])\n","    \"\"\"\n","\n","    def __init__(self, vocab: list[str]):\n","        self.vocab = vocab\n","        self.separentorMatcher = re.compile(r'[ -\\/:-@\\[-`\\{-~]')\n","\n","    def get_words_info(self, sentence: str, tokens: list[int]) -> list[tuple[int, int]]:\n","        \"\"\"\n","        Return the index and length of each word in the sentence.\n","        :param sentence: the sentence string\n","        :param tokens: the token representation of the sentence\n","        :returns: list of tuple that contains the word's start and end indexes\n","        \"\"\"\n","        sentence_length = len(sentence)\n","        max_token_idx = len(tokens) - 1\n","        words_info = []\n","        char_ptr = 0\n","        for token_idx, token in enumerate(tokens):\n","            #find the word length of the token\n","            if token == 101: # [CLS]\n","                char_end_ptr = char_ptr\n","            elif token == 102: # [SEP]\n","                break\n","            elif token == 100: # word is [UNK]\n","                #find next word position\n","                if max_token_idx > token_idx:\n","                    next_token = tokens[token_idx+1]\n","                    if next_token == 100:\n","                        #find separent symbol\n","                        match = self.separentorMatcher.search(sentence, char_ptr)\n","                        if match is not None:\n","                            char_end_ptr = match.start()\n","                        else:\n","                            char_end_ptr = sentence_length\n","                    else:\n","                        next_word = self.get_word(next_token)\n","                        char_end_ptr = sentence.index(next_word, char_ptr)\n","                else:\n","                    char_end_ptr = sentence_length\n","            else:\n","                word = self.get_word(token)\n","                char_end_ptr = char_ptr + len(word)\n","            #store the corresponding word position and length of the token\n","            words_info.append((char_ptr, char_end_ptr))\n","            char_ptr = char_end_ptr\n","        return words_info\n","\n","    def get_word(self, token: int) -> str:\n","        \"\"\"\n","        Convert given token back to the word piece\n","        \"\"\"\n","        return self.vocab[token].replace('##', '')\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":304,"status":"ok","timestamp":1703309608903,"user":{"displayName":"Andy Ching","userId":"04259283919464139952"},"user_tz":-480},"id":"ydRoIfyHmDJa"},"outputs":[],"source":["sentenceInfoParser = SentenceInfoPraser(vocab)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1097,"status":"ok","timestamp":1703310046941,"user":{"displayName":"Andy Ching","userId":"04259283919464139952"},"user_tz":-480},"id":"CBVdFbIdmDJa"},"outputs":[],"source":["sentences = [\n","    # \"古語有云：等到潮水退了，就知道誰沒穿褲子。\",\n","    # \"古語有云：當地天氣嚴寒，室內亦只有兩、三度，不少衣衫單簿的女歌手都冷得發抖。\",\n","    # \"古語有云：《三國演義》一書敍述了魏、蜀、吳三國之間復雜的爭鬥。\",\n","    # \"古語有云：摧眠治療早於 1958 年己被美國醫學會 (AMA) 宣佈為正式的精神治療方法之一。\",\n","    # \"古語有云：香港大球場舉行了一連三天的國際性球賽，吸引了世界各地球迷蜂湧而至。\",\n","    # \"古語有云：請問我於遞交申請表和會費後，何時才會收到會員卡和優惠贈卷呢？\",\n","    # \"古語有云：不少人認為燈迷很難猜，其實只要我們掌握其法，就很容易猜到答案的。\",\n","    # \"古語有云：他的文章內容混亂，令人看得頭昏腦漲。\",\n","    # \"古語有云：他的表現輕佻浮燥，惹人討厭。\",\n","    # \"古語有云：他對偶像的鐘愛，已到了盲目的地步。\",\n","    # \"古語有云：正確和錯誤，或真與假，可以算是邏輯裏面最基本的慨念。\",\n","    # \"【本文獲授權轉載。】\",\n","    # \"案件編號：CCDI-602/2016\",\n","    # \"【林綸詩其他文章：】\",\n","    # \"• 宣布天厨違反第一行為守則\",\n","    # \"• 向天厨施加罰款\",\n","    # \"• 禁止天厨日後從事違反該守則的相同行為\",\n","    # \"• 天厨須推行有效的合規計劃\",\n","    # \"• 向天厨收取競委會的訟費及調查費用\",\n","    # \"↓↓↓單位直撃↓↓↓\",\n","    # \"👇🏻即看3款總評分5星產品👇🏻\",\n","    # \"葉劉：難斷定11月會否放寬至「0+7」\",\n","    # \"👉\",\n","    # \"PCAOB必須能夠訪問所有在其註冊的公眾會計師事務所的審計文件並選擇任何審計業務，而非僅僅部分事務所或部分業務才能夠在內地和香港進行全面的檢查和調查。\",\n","    # \"PCAOB將通知審計公司其檢查計畫，包括具體的業務。PCAOB檢查員將於9月中旬前在香港開始他們的檢查工作。\",\n","    # \"HKZO有個重點一定要注意，可能會分散注意力。\",\n","    # \"入伙後一家四口陸續收到由「The Watcher」寄來的恐怖信件，遇上不友善的鄰居擅闖家中，並躲再食物升降機。\",\n","    # \"保險公司Allstate對這兩款產品進行了跌落測試，測試設備為Dropbot，保證拍出認為因素干擾，最終得到的結果是：iPhone 14 Plus更耐摔。\",\n","    \"影片只有5秒，但已經有15萬次點擊，歌迷相當關注，亦有人讚賞Jennie敬業，馬上就重新站起來表演，有時常看演出的網民表示，這應該跟耳機漏電有關，更表示「已經不是第一次」，覺得大會跟共公司要改善，照片歌手安全。\",\n","\n","    # \"她擔心得笑不停。\",\n","    # \"她開心得笑不停。\",\n","]\n","\n","outputs = model(tf.constant(sentences))"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1703310048973,"user":{"displayName":"Andy Ching","userId":"04259283919464139952"},"user_tz":-480},"id":"127ZkzWfmDJb","outputId":"77676777-3428-4e6a-ed4d-8e09fff420a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["#01(2512)     影(  4.39%):   。( 27.04%)\n","#02(4275)     片(100.00%):   片(100.00%)\n","#03(1372)     只( 99.98%):   只( 99.98%)\n","#04(3300)     有(100.00%):   有(100.00%)\n","#05(126)     5( 99.96%):   5( 99.96%)\n","#06(4907)     秒(100.00%):   秒(100.00%)\n","#07(8024)     ，( 99.99%):   ，( 99.99%)\n","#08(852)     但( 99.99%):   但( 99.99%)\n","#09(2347)     已(100.00%):   已(100.00%)\n","#10(5195)     經(100.00%):   經(100.00%)\n","#11(3300)     有(100.00%):   有(100.00%)\n","#12(8115)    15( 99.97%):  15( 99.97%)\n","#13(5857)     萬(100.00%):   萬(100.00%)\n","#14(3613)     次(100.00%):   次(100.00%)\n","#15(7953)     點(100.00%):   點(100.00%)\n","#16(3080)     擊(100.00%):   擊(100.00%)\n","#17(8024)     ，( 99.96%):   ，( 99.96%)\n","#18(3625)     歌(100.00%):   歌(100.00%)\n","#19(6837)     迷(100.00%):   迷(100.00%)\n","#20(4685)     相(100.00%):   相(100.00%)\n","#21(4534)     當(100.00%):   當(100.00%)\n","#22(7302)     關(100.00%):   關(100.00%)\n","#23(3800)     注(100.00%):   注(100.00%)\n","#24(8024)     ，( 99.99%):   ，( 99.99%)\n","#25(771)     亦( 99.99%):   亦( 99.99%)\n","#26(3300)     有(100.00%):   有(100.00%)\n","#27(782)     人(100.00%):   人(100.00%)\n","#28(6367)     讚(100.00%):   讚(100.00%)\n","#29(6542)     賞(100.00%):   賞(100.00%)\n","#30(100) Jennie( 99.96%): [UNK]( 99.96%)\n","#31(3143)     敬( 99.96%):   敬( 99.96%)\n","#32(3511)     業(100.00%):   業(100.00%)\n","#33(8024)     ，(100.00%):   ，(100.00%)\n","#34(7679)     馬(100.00%):   馬(100.00%)\n","#35(677)     上(100.00%):   上(100.00%)\n","#36(2218)     就( 99.99%):   就( 99.99%)\n","#37(7028)     重( 99.99%):   重( 99.99%)\n","#38(3173)     新( 99.99%):   新( 99.99%)\n","#39(4991)     站(100.00%):   站(100.00%)\n","#40(6629)     起(100.00%):   起(100.00%)\n","#41(889)     來(100.00%):   來(100.00%)\n","#42(6134)     表(100.00%):   表(100.00%)\n","#43(4028)     演( 99.96%):   演( 99.96%)\n","#44(8024)     ，( 99.93%):   ，( 99.93%)\n","#45(3300)     有(100.00%):   有(100.00%)\n","#46(3229)     時( 99.49%):   時( 99.49%)\n","#47(2382)     常( 99.94%):   常( 99.94%)\n","#48(4692)     看(100.00%):   看(100.00%)\n","#49(4028)     演(100.00%):   演(100.00%)\n","#50(1139)     出(100.00%):   出(100.00%)\n","#51(4638)     的(100.00%):   的(100.00%)\n","#52(5206)     網(100.00%):   網(100.00%)\n","#53(3696)     民(100.00%):   民(100.00%)\n","#54(6134)     表(100.00%):   表(100.00%)\n","#55(4850)     示(100.00%):   示(100.00%)\n","#56(8024)     ，(100.00%):   ，(100.00%)\n","#57(6857)     這(100.00%):   這(100.00%)\n","#58(2746)     應(100.00%):   應(100.00%)\n","#59(6283)     該(100.00%):   該(100.00%)\n","#60(6656)     跟( 99.99%):   跟( 99.99%)\n","#61(5455)     耳(100.00%):   耳(100.00%)\n","#62(3582)     機(100.00%):   機(100.00%)\n","#63(4026)     漏(100.00%):   漏(100.00%)\n","#64(7442)     電(100.00%):   電(100.00%)\n","#65(3300)     有(100.00%):   有(100.00%)\n","#66(7302)     關(100.00%):   關(100.00%)\n","#67(8024)     ，(100.00%):   ，(100.00%)\n","#68(3291)     更( 99.97%):   更( 99.97%)\n","#69(6134)     表(100.00%):   表(100.00%)\n","#70(4850)     示(100.00%):   示(100.00%)\n","#71(519)     「(100.00%):   「(100.00%)\n","#72(2347)     已(100.00%):   已(100.00%)\n","#73(5195)     經(100.00%):   經(100.00%)\n","#74(679)     不( 99.99%):   不( 99.99%)\n","#75(3221)     是(100.00%):   是(100.00%)\n","#76(5018)     第(100.00%):   第(100.00%)\n","#77(671)     一(100.00%):   一(100.00%)\n","#78(3613)     次(100.00%):   次(100.00%)\n","#79(520)     」(100.00%):   」(100.00%)\n","#80(8024)     ，(100.00%):   ，(100.00%)\n","#81(6221)     覺(100.00%):   覺(100.00%)\n","#82(2533)     得(100.00%):   得(100.00%)\n","#83(1920)     大( 99.93%):   大( 99.93%)\n","#84(3298)     會( 99.96%):   會( 99.96%)\n","#85(6656)     跟(  0.39%):   公( 93.48%)\n","#86(1066)     共( 33.90%):   共( 33.90%)\n","#87(1062)     公( 99.84%):   公( 99.84%)\n","#88(1385)     司(100.00%):   司(100.00%)\n","#89(6206)     要(100.00%):   要(100.00%)\n","#90(3121)     改(100.00%):   改(100.00%)\n","#91(1587)     善(100.00%):   善(100.00%)\n","#92(8024)     ，( 99.87%):   ，( 99.87%)\n","#93(4212)     照( 94.10%):   照( 94.10%)\n","#94(4275)     片( 97.66%):   片( 97.66%)\n","#95(3625)     歌( 99.88%):   歌( 99.88%)\n","#96(2797)     手( 99.68%):   手( 99.68%)\n","#97(2128)     安(100.00%):   安(100.00%)\n","#98(1059)     全(100.00%):   全(100.00%)\n","#99(511)     。( 99.99%):   。( 99.99%)\n"]}],"source":["for sentence, char_tokens, char_porbs, top1_tokens, top1_probs in zip(\n","        sentences,\n","        outputs[\"char_token\"], outputs[\"char_prob\"], outputs[\"top1_token\"],\n","        outputs[\"top1_prob\"]):\n","    tokens = list(char_tokens.numpy())\n","    tokens = tokens[:tokens.index(0)] #crop padding\n","    words_info = sentenceInfoParser.get_words_info(sentence, tokens)\n","    for token_idx, char_token in enumerate(tokens):\n","        if char_token == 101:\n","            continue\n","        if char_token == 102:\n","            break\n","        else:\n","            word_start_idx, word_end_idx = words_info[token_idx]\n","            word = sentence[word_start_idx:word_end_idx]\n","            char = vocab[char_token]\n","            char_prob = char_porbs[token_idx]\n","            top1_token = top1_tokens[token_idx]\n","            top1_char = vocab[top1_token]\n","            top1_prob = top1_probs[token_idx]\n","            print(f\"#{token_idx:0>2}({char_token}) {word:>5}({char_prob:>7.2%}): {top1_char:>3}({top1_prob:>7.2%})\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.9.13 64-bit ('3.9.13')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"42285567d25b4285133252383bb21604aee132a4bd119b875b859f48019e7f4e"}}},"nbformat":4,"nbformat_minor":0}
