{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15280,"status":"ok","timestamp":1703309542337,"user":{"displayName":"Andy Ching","userId":"04259283919464139952"},"user_tz":-480},"id":"Yja0IANEJ-qP","outputId":"07a6f80c-ba0a-4584-afd7-7024f3f86bb5"},"outputs":[{"name":"stderr","output_type":"stream","text":["UsageError: Line magic function `%pip3` not found.\n"]}],"source":["%pip install tensorflow_text tensorflow_hub"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":9574,"status":"ok","timestamp":1703309566203,"user":{"displayName":"Andy Ching","userId":"04259283919464139952"},"user_tz":-480},"id":"DvXu-ZDImDJX"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_text as text\n","import tensorflow_hub as hub\n","import pandas as pd\n","import numpy as np\n","from functools import cache\n","import re"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":307,"status":"ok","timestamp":1703309571089,"user":{"displayName":"Andy Ching","userId":"04259283919464139952"},"user_tz":-480},"id":"blpUwVOGmDJZ"},"outputs":[],"source":["class TypoDetector:\n","\n","    def __init__(\n","            self,\n","            preprocessor_handle=\"https://tfhub.dev/tensorflow/bert_zh_preprocess/3\",\n","            encoder_handle=\"https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12/4\",\n","            seq_length: int = 128,\n","            trainable: bool = True) -> None:\n","        self.preprocessor_handle = preprocessor_handle\n","        self.encoder_handle = encoder_handle\n","        self.seq_length = seq_length\n","        self.trainable = trainable\n","        self.preprocessor = hub.load(preprocessor_handle)\n","        self.encoder = hub.load(encoder_handle)\n","\n","    @cache\n","    def get_vocab(self) -> list[str]:\n","        \"\"\"\n","        Get the vocab dictionary from encoder\n","        :returns: the vocab list\n","        \"\"\"\n","        vocab_filepath = self.encoder.vocab_file.asset_path.numpy().decode(\n","            \"utf-8\")\n","        with open(vocab_filepath, 'r') as f:\n","            return [vocab[:-1] for vocab in f]\n","\n","    @cache\n","    def get_vocab_size(self) -> int:\n","        \"\"\"\n","        Get the length of the vocab list\n","        :returns: vocab size\n","        \"\"\"\n","        return self.preprocessor.tokenize.get_special_tokens_dict(\n","        )['vocab_size']\n","\n","    def get_tokenizer(self) -> hub.KerasLayer:\n","        \"\"\"\n","        Get the tokenizer as the preprocessor of the bert model\n","        :returns: the tokenizer\n","        \"\"\"\n","        return hub.KerasLayer(self.preprocessor.tokenize)\n","\n","    def get_model(self) -> tf.keras.Model:\n","        \"\"\"\n","        Compose and return our detector model\n","        :returns: the detector model\n","        \"\"\"\n","        # input layer\n","        text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n","        # tokenizer layer\n","        tokenizer = self.get_tokenizer()\n","        # pack-inputs layer\n","        bert_pack_inputs = hub.KerasLayer(\n","            self.preprocessor.bert_pack_inputs,\n","            arguments={\"seq_length\": tf.constant(self.seq_length)})\n","        # mlm layer\n","        mlm_layer = hub.KerasLayer(self.encoder.mlm, trainable=self.trainable)\n","        # string input to tokens\n","        tokenized_input = tokenizer(text_input)\n","        # tokens to bert encoder inputs\n","        encoder_inputs = bert_pack_inputs([tokenized_input])\n","        # bert encoder inputs to mlm inputs\n","        mlm_inputs = self.__to_mlm_inputs(encoder_inputs)\n","        # consume mlm inputs and produce corresponding outputs by the mlm model\n","        mlm_outputs = mlm_layer(mlm_inputs)\n","        # convert mlm outputs to probabilities of each char of the input\n","        logit_probs = self.__to_logit_probs(mlm_outputs)\n","        char_prob = self.__to_char_prob(encoder_inputs, logit_probs)\n","        top1_prob, top1_token = self.__to_topk_probs(logit_probs, 1)\n","        top1_prob = tf.squeeze(top1_prob, axis=-1)\n","        top1_token = tf.squeeze(top1_token, axis=-1)\n","        char_token = encoder_inputs['input_word_ids']\n","        outputs = {\n","            'char_token': char_token,\n","            'char_prob': char_prob,\n","            'top1_token': top1_token,\n","            'top1_prob': top1_prob,\n","        }\n","        # pack the layers as tf model\n","        return tf.keras.Model(text_input, outputs)\n","\n","    def __to_logit_probs(self, mlm_outputs: dict) -> tf.Tensor:\n","        \"\"\"\n","        Convert mlm outputs to the probabilities of each char in vocab\n","        :returns: shape=(batch_size, seq_length, vocab_size)\n","        \"\"\"\n","        return tf.keras.layers.Softmax()(mlm_outputs[\"mlm_logits\"])\n","\n","    def __to_topk_probs(self, logit_probs, k: int = 1) -> list[tf.Tensor]:\n","        \"\"\"\n","        Convert mlm outputs to the indices of top K chars\n","        :param logit_probs: returns of self.__to_logit_probs()\n","        :param k: number of items at the top most to take, optional\n","        :returns: list(values=[shape=(batch_size, seq_length, k)], indices=[shape=(batch_size, seq_length, k)])\n","        \"\"\"\n","        return tf.math.top_k(logit_probs, k)\n","\n","    def __to_char_prob(self, encoder_inputs: dict, logit_probs) -> tf.Tensor:\n","        \"\"\"\n","        Convert the mlm outputs to the probabilities of each char of sentences\n","        :param encoder_inputs: inputs for the bert model\n","        :param logit_probs: returns of self.__to_logit_probs()\n","        :returns: shape=(batch_size, seq_length)\n","        \"\"\"\n","        vocab_size = self.get_vocab_size()\n","        one_hot_token_ids = tf.one_hot(encoder_inputs[\"input_word_ids\"],\n","                                       vocab_size)\n","        return tf.reduce_max(tf.multiply(logit_probs, one_hot_token_ids), -1)\n","\n","    def __to_mlm_inputs(self, encoder_inputs: dict) -> dict[str, tf.Tensor]:\n","        \"\"\"\n","        Convert bert encoder inputs input mlm inputs\n","        :returns: the corresponding inputs for bert's mlm model\n","        \"\"\"\n","        masked_lm_positions = tf.multiply(\n","            tf.ones_like(encoder_inputs[\"input_word_ids\"], dtype=tf.int32),\n","            tf.transpose(tf.range(0, self.seq_length)))\n","        return {\n","            \"input_word_ids\": encoder_inputs[\"input_word_ids\"],\n","            \"input_mask\": encoder_inputs[\"input_mask\"],\n","            \"input_type_ids\": encoder_inputs[\"input_type_ids\"],\n","            \"masked_lm_positions\": masked_lm_positions,\n","        }\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":24926,"status":"ok","timestamp":1703309600794,"user":{"displayName":"Andy Ching","userId":"04259283919464139952"},"user_tz":-480},"id":"Y1q4XZE0mDJa"},"outputs":[],"source":["detector = TypoDetector()\n","model = detector.get_model()\n","vocab = detector.get_vocab()"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":287,"status":"ok","timestamp":1703309603536,"user":{"displayName":"Andy Ching","userId":"04259283919464139952"},"user_tz":-480},"id":"ToH3X9BimDJa"},"outputs":[],"source":["class SentenceInfoPraser:\n","    \"\"\"\n","    Example:\n","    sentenceInfoParser = SentenceInfoPraser(vocab)\n","    sentenceInfoParser.parse_sentences([\"ã€ŒStar Warã€å¾ˆå¥½çœ‹ã€‚\"])\n","    \"\"\"\n","\n","    def __init__(self, vocab: list[str]):\n","        self.vocab = vocab\n","        self.separentorMatcher = re.compile(r'[ -\\/:-@\\[-`\\{-~]')\n","\n","    def get_words_info(self, sentence: str, tokens: list[int]) -> list[tuple[int, int]]:\n","        \"\"\"\n","        Return the index and length of each word in the sentence.\n","        :param sentence: the sentence string\n","        :param tokens: the token representation of the sentence\n","        :returns: list of tuple that contains the word's start and end indexes\n","        \"\"\"\n","        sentence_length = len(sentence)\n","        max_token_idx = len(tokens) - 1\n","        words_info = []\n","        char_ptr = 0\n","        for token_idx, token in enumerate(tokens):\n","            #find the word length of the token\n","            if token == 101: # [CLS]\n","                char_end_ptr = char_ptr\n","            elif token == 102: # [SEP]\n","                break\n","            elif token == 100: # word is [UNK]\n","                #find next word position\n","                if max_token_idx > token_idx:\n","                    next_token = tokens[token_idx+1]\n","                    if next_token == 100:\n","                        #find separent symbol\n","                        match = self.separentorMatcher.search(sentence, char_ptr)\n","                        if match is not None:\n","                            char_end_ptr = match.start()\n","                        else:\n","                            char_end_ptr = sentence_length\n","                    else:\n","                        next_word = self.get_word(next_token)\n","                        char_end_ptr = sentence.index(next_word, char_ptr)\n","                else:\n","                    char_end_ptr = sentence_length\n","            else:\n","                word = self.get_word(token)\n","                char_end_ptr = char_ptr + len(word)\n","            #store the corresponding word position and length of the token\n","            words_info.append((char_ptr, char_end_ptr))\n","            char_ptr = char_end_ptr\n","        return words_info\n","\n","    def get_word(self, token: int) -> str:\n","        \"\"\"\n","        Convert given token back to the word piece\n","        \"\"\"\n","        return self.vocab[token].replace('##', '')\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":304,"status":"ok","timestamp":1703309608903,"user":{"displayName":"Andy Ching","userId":"04259283919464139952"},"user_tz":-480},"id":"ydRoIfyHmDJa"},"outputs":[],"source":["sentenceInfoParser = SentenceInfoPraser(vocab)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1097,"status":"ok","timestamp":1703310046941,"user":{"displayName":"Andy Ching","userId":"04259283919464139952"},"user_tz":-480},"id":"CBVdFbIdmDJa"},"outputs":[],"source":["sentences = [\n","    # \"å¤èªžæœ‰äº‘ï¼šç­‰åˆ°æ½®æ°´é€€äº†ï¼Œå°±çŸ¥é“èª°æ²’ç©¿è¤²å­ã€‚\",\n","    # \"å¤èªžæœ‰äº‘ï¼šç•¶åœ°å¤©æ°£åš´å¯’ï¼Œå®¤å…§äº¦åªæœ‰å…©ã€ä¸‰åº¦ï¼Œä¸å°‘è¡£è¡«å–®ç°¿çš„å¥³æ­Œæ‰‹éƒ½å†·å¾—ç™¼æŠ–ã€‚\",\n","    # \"å¤èªžæœ‰äº‘ï¼šã€Šä¸‰åœ‹æ¼”ç¾©ã€‹ä¸€æ›¸æ•è¿°äº†é­ã€èœ€ã€å³ä¸‰åœ‹ä¹‹é–“å¾©é›œçš„çˆ­é¬¥ã€‚\",\n","    # \"å¤èªžæœ‰äº‘ï¼šæ‘§çœ æ²»ç™‚æ—©æ–¼ 1958 å¹´å·±è¢«ç¾Žåœ‹é†«å­¸æœƒ (AMA) å®£ä½ˆç‚ºæ­£å¼çš„ç²¾ç¥žæ²»ç™‚æ–¹æ³•ä¹‹ä¸€ã€‚\",\n","    # \"å¤èªžæœ‰äº‘ï¼šé¦™æ¸¯å¤§çƒå ´èˆ‰è¡Œäº†ä¸€é€£ä¸‰å¤©çš„åœ‹éš›æ€§çƒè³½ï¼Œå¸å¼•äº†ä¸–ç•Œå„åœ°çƒè¿·èœ‚æ¹§è€Œè‡³ã€‚\",\n","    # \"å¤èªžæœ‰äº‘ï¼šè«‹å•æˆ‘æ–¼éžäº¤ç”³è«‹è¡¨å’Œæœƒè²»å¾Œï¼Œä½•æ™‚æ‰æœƒæ”¶åˆ°æœƒå“¡å¡å’Œå„ªæƒ è´ˆå·å‘¢ï¼Ÿ\",\n","    # \"å¤èªžæœ‰äº‘ï¼šä¸å°‘äººèªç‚ºç‡ˆè¿·å¾ˆé›£çŒœï¼Œå…¶å¯¦åªè¦æˆ‘å€‘æŽŒæ¡å…¶æ³•ï¼Œå°±å¾ˆå®¹æ˜“çŒœåˆ°ç­”æ¡ˆçš„ã€‚\",\n","    # \"å¤èªžæœ‰äº‘ï¼šä»–çš„æ–‡ç« å…§å®¹æ··äº‚ï¼Œä»¤äººçœ‹å¾—é ­æ˜è…¦æ¼²ã€‚\",\n","    # \"å¤èªžæœ‰äº‘ï¼šä»–çš„è¡¨ç¾è¼•ä½»æµ®ç‡¥ï¼Œæƒ¹äººè¨ŽåŽ­ã€‚\",\n","    # \"å¤èªžæœ‰äº‘ï¼šä»–å°å¶åƒçš„é˜æ„›ï¼Œå·²åˆ°äº†ç›²ç›®çš„åœ°æ­¥ã€‚\",\n","    # \"å¤èªžæœ‰äº‘ï¼šæ­£ç¢ºå’ŒéŒ¯èª¤ï¼Œæˆ–çœŸèˆ‡å‡ï¼Œå¯ä»¥ç®—æ˜¯é‚è¼¯è£é¢æœ€åŸºæœ¬çš„æ…¨å¿µã€‚\",\n","    # \"ã€æœ¬æ–‡ç²æŽˆæ¬Šè½‰è¼‰ã€‚ã€‘\",\n","    # \"æ¡ˆä»¶ç·¨è™Ÿï¼šCCDI-602/2016\",\n","    # \"ã€æž—ç¶¸è©©å…¶ä»–æ–‡ç« ï¼šã€‘\",\n","    # \"â€¢ å®£å¸ƒå¤©åŽ¨é•åç¬¬ä¸€è¡Œç‚ºå®ˆå‰‡\",\n","    # \"â€¢ å‘å¤©åŽ¨æ–½åŠ ç½°æ¬¾\",\n","    # \"â€¢ ç¦æ­¢å¤©åŽ¨æ—¥å¾Œå¾žäº‹é•åè©²å®ˆå‰‡çš„ç›¸åŒè¡Œç‚º\",\n","    # \"â€¢ å¤©åŽ¨é ˆæŽ¨è¡Œæœ‰æ•ˆçš„åˆè¦è¨ˆåŠƒ\",\n","    # \"â€¢ å‘å¤©åŽ¨æ”¶å–ç«¶å§”æœƒçš„è¨Ÿè²»åŠèª¿æŸ¥è²»ç”¨\",\n","    # \"â†“â†“â†“å–®ä½ç›´æ’ƒâ†“â†“â†“\",\n","    # \"ðŸ‘‡ðŸ»å³çœ‹3æ¬¾ç¸½è©•åˆ†5æ˜Ÿç”¢å“ðŸ‘‡ðŸ»\",\n","    # \"è‘‰åŠ‰ï¼šé›£æ–·å®š11æœˆæœƒå¦æ”¾å¯¬è‡³ã€Œ0+7ã€\",\n","    # \"ðŸ‘‰\",\n","    # \"PCAOBå¿…é ˆèƒ½å¤ è¨ªå•æ‰€æœ‰åœ¨å…¶è¨»å†Šçš„å…¬çœ¾æœƒè¨ˆå¸«äº‹å‹™æ‰€çš„å¯©è¨ˆæ–‡ä»¶ä¸¦é¸æ“‡ä»»ä½•å¯©è¨ˆæ¥­å‹™ï¼Œè€Œéžåƒ…åƒ…éƒ¨åˆ†äº‹å‹™æ‰€æˆ–éƒ¨åˆ†æ¥­å‹™æ‰èƒ½å¤ åœ¨å…§åœ°å’Œé¦™æ¸¯é€²è¡Œå…¨é¢çš„æª¢æŸ¥å’Œèª¿æŸ¥ã€‚\",\n","    # \"PCAOBå°‡é€šçŸ¥å¯©è¨ˆå…¬å¸å…¶æª¢æŸ¥è¨ˆç•«ï¼ŒåŒ…æ‹¬å…·é«”çš„æ¥­å‹™ã€‚PCAOBæª¢æŸ¥å“¡å°‡æ–¼9æœˆä¸­æ—¬å‰åœ¨é¦™æ¸¯é–‹å§‹ä»–å€‘çš„æª¢æŸ¥å·¥ä½œã€‚\",\n","    # \"HKZOæœ‰å€‹é‡é»žä¸€å®šè¦æ³¨æ„ï¼Œå¯èƒ½æœƒåˆ†æ•£æ³¨æ„åŠ›ã€‚\",\n","    # \"å…¥ä¼™å¾Œä¸€å®¶å››å£é™¸çºŒæ”¶åˆ°ç”±ã€ŒThe Watcherã€å¯„ä¾†çš„ææ€–ä¿¡ä»¶ï¼Œé‡ä¸Šä¸å‹å–„çš„é„°å±…æ“…é—–å®¶ä¸­ï¼Œä¸¦èº²å†é£Ÿç‰©å‡é™æ©Ÿã€‚\",\n","    # \"ä¿éšªå…¬å¸Allstateå°é€™å…©æ¬¾ç”¢å“é€²è¡Œäº†è·Œè½æ¸¬è©¦ï¼Œæ¸¬è©¦è¨­å‚™ç‚ºDropbotï¼Œä¿è­‰æ‹å‡ºèªç‚ºå› ç´ å¹²æ“¾ï¼Œæœ€çµ‚å¾—åˆ°çš„çµæžœæ˜¯ï¼šiPhone 14 Plusæ›´è€æ‘”ã€‚\",\n","    \"å½±ç‰‡åªæœ‰5ç§’ï¼Œä½†å·²ç¶“æœ‰15è¬æ¬¡é»žæ“Šï¼Œæ­Œè¿·ç›¸ç•¶é—œæ³¨ï¼Œäº¦æœ‰äººè®šè³žJennieæ•¬æ¥­ï¼Œé¦¬ä¸Šå°±é‡æ–°ç«™èµ·ä¾†è¡¨æ¼”ï¼Œæœ‰æ™‚å¸¸çœ‹æ¼”å‡ºçš„ç¶²æ°‘è¡¨ç¤ºï¼Œé€™æ‡‰è©²è·Ÿè€³æ©Ÿæ¼é›»æœ‰é—œï¼Œæ›´è¡¨ç¤ºã€Œå·²ç¶“ä¸æ˜¯ç¬¬ä¸€æ¬¡ã€ï¼Œè¦ºå¾—å¤§æœƒè·Ÿå…±å…¬å¸è¦æ”¹å–„ï¼Œç…§ç‰‡æ­Œæ‰‹å®‰å…¨ã€‚\",\n","\n","    # \"å¥¹æ“”å¿ƒå¾—ç¬‘ä¸åœã€‚\",\n","    # \"å¥¹é–‹å¿ƒå¾—ç¬‘ä¸åœã€‚\",\n","]\n","\n","outputs = model(tf.constant(sentences))"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1703310048973,"user":{"displayName":"Andy Ching","userId":"04259283919464139952"},"user_tz":-480},"id":"127ZkzWfmDJb","outputId":"77676777-3428-4e6a-ed4d-8e09fff420a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["#01(2512)     å½±(  4.39%):   ã€‚( 27.04%)\n","#02(4275)     ç‰‡(100.00%):   ç‰‡(100.00%)\n","#03(1372)     åª( 99.98%):   åª( 99.98%)\n","#04(3300)     æœ‰(100.00%):   æœ‰(100.00%)\n","#05(126)     5( 99.96%):   5( 99.96%)\n","#06(4907)     ç§’(100.00%):   ç§’(100.00%)\n","#07(8024)     ï¼Œ( 99.99%):   ï¼Œ( 99.99%)\n","#08(852)     ä½†( 99.99%):   ä½†( 99.99%)\n","#09(2347)     å·²(100.00%):   å·²(100.00%)\n","#10(5195)     ç¶“(100.00%):   ç¶“(100.00%)\n","#11(3300)     æœ‰(100.00%):   æœ‰(100.00%)\n","#12(8115)    15( 99.97%):  15( 99.97%)\n","#13(5857)     è¬(100.00%):   è¬(100.00%)\n","#14(3613)     æ¬¡(100.00%):   æ¬¡(100.00%)\n","#15(7953)     é»ž(100.00%):   é»ž(100.00%)\n","#16(3080)     æ“Š(100.00%):   æ“Š(100.00%)\n","#17(8024)     ï¼Œ( 99.96%):   ï¼Œ( 99.96%)\n","#18(3625)     æ­Œ(100.00%):   æ­Œ(100.00%)\n","#19(6837)     è¿·(100.00%):   è¿·(100.00%)\n","#20(4685)     ç›¸(100.00%):   ç›¸(100.00%)\n","#21(4534)     ç•¶(100.00%):   ç•¶(100.00%)\n","#22(7302)     é—œ(100.00%):   é—œ(100.00%)\n","#23(3800)     æ³¨(100.00%):   æ³¨(100.00%)\n","#24(8024)     ï¼Œ( 99.99%):   ï¼Œ( 99.99%)\n","#25(771)     äº¦( 99.99%):   äº¦( 99.99%)\n","#26(3300)     æœ‰(100.00%):   æœ‰(100.00%)\n","#27(782)     äºº(100.00%):   äºº(100.00%)\n","#28(6367)     è®š(100.00%):   è®š(100.00%)\n","#29(6542)     è³ž(100.00%):   è³ž(100.00%)\n","#30(100) Jennie( 99.96%): [UNK]( 99.96%)\n","#31(3143)     æ•¬( 99.96%):   æ•¬( 99.96%)\n","#32(3511)     æ¥­(100.00%):   æ¥­(100.00%)\n","#33(8024)     ï¼Œ(100.00%):   ï¼Œ(100.00%)\n","#34(7679)     é¦¬(100.00%):   é¦¬(100.00%)\n","#35(677)     ä¸Š(100.00%):   ä¸Š(100.00%)\n","#36(2218)     å°±( 99.99%):   å°±( 99.99%)\n","#37(7028)     é‡( 99.99%):   é‡( 99.99%)\n","#38(3173)     æ–°( 99.99%):   æ–°( 99.99%)\n","#39(4991)     ç«™(100.00%):   ç«™(100.00%)\n","#40(6629)     èµ·(100.00%):   èµ·(100.00%)\n","#41(889)     ä¾†(100.00%):   ä¾†(100.00%)\n","#42(6134)     è¡¨(100.00%):   è¡¨(100.00%)\n","#43(4028)     æ¼”( 99.96%):   æ¼”( 99.96%)\n","#44(8024)     ï¼Œ( 99.93%):   ï¼Œ( 99.93%)\n","#45(3300)     æœ‰(100.00%):   æœ‰(100.00%)\n","#46(3229)     æ™‚( 99.49%):   æ™‚( 99.49%)\n","#47(2382)     å¸¸( 99.94%):   å¸¸( 99.94%)\n","#48(4692)     çœ‹(100.00%):   çœ‹(100.00%)\n","#49(4028)     æ¼”(100.00%):   æ¼”(100.00%)\n","#50(1139)     å‡º(100.00%):   å‡º(100.00%)\n","#51(4638)     çš„(100.00%):   çš„(100.00%)\n","#52(5206)     ç¶²(100.00%):   ç¶²(100.00%)\n","#53(3696)     æ°‘(100.00%):   æ°‘(100.00%)\n","#54(6134)     è¡¨(100.00%):   è¡¨(100.00%)\n","#55(4850)     ç¤º(100.00%):   ç¤º(100.00%)\n","#56(8024)     ï¼Œ(100.00%):   ï¼Œ(100.00%)\n","#57(6857)     é€™(100.00%):   é€™(100.00%)\n","#58(2746)     æ‡‰(100.00%):   æ‡‰(100.00%)\n","#59(6283)     è©²(100.00%):   è©²(100.00%)\n","#60(6656)     è·Ÿ( 99.99%):   è·Ÿ( 99.99%)\n","#61(5455)     è€³(100.00%):   è€³(100.00%)\n","#62(3582)     æ©Ÿ(100.00%):   æ©Ÿ(100.00%)\n","#63(4026)     æ¼(100.00%):   æ¼(100.00%)\n","#64(7442)     é›»(100.00%):   é›»(100.00%)\n","#65(3300)     æœ‰(100.00%):   æœ‰(100.00%)\n","#66(7302)     é—œ(100.00%):   é—œ(100.00%)\n","#67(8024)     ï¼Œ(100.00%):   ï¼Œ(100.00%)\n","#68(3291)     æ›´( 99.97%):   æ›´( 99.97%)\n","#69(6134)     è¡¨(100.00%):   è¡¨(100.00%)\n","#70(4850)     ç¤º(100.00%):   ç¤º(100.00%)\n","#71(519)     ã€Œ(100.00%):   ã€Œ(100.00%)\n","#72(2347)     å·²(100.00%):   å·²(100.00%)\n","#73(5195)     ç¶“(100.00%):   ç¶“(100.00%)\n","#74(679)     ä¸( 99.99%):   ä¸( 99.99%)\n","#75(3221)     æ˜¯(100.00%):   æ˜¯(100.00%)\n","#76(5018)     ç¬¬(100.00%):   ç¬¬(100.00%)\n","#77(671)     ä¸€(100.00%):   ä¸€(100.00%)\n","#78(3613)     æ¬¡(100.00%):   æ¬¡(100.00%)\n","#79(520)     ã€(100.00%):   ã€(100.00%)\n","#80(8024)     ï¼Œ(100.00%):   ï¼Œ(100.00%)\n","#81(6221)     è¦º(100.00%):   è¦º(100.00%)\n","#82(2533)     å¾—(100.00%):   å¾—(100.00%)\n","#83(1920)     å¤§( 99.93%):   å¤§( 99.93%)\n","#84(3298)     æœƒ( 99.96%):   æœƒ( 99.96%)\n","#85(6656)     è·Ÿ(  0.39%):   å…¬( 93.48%)\n","#86(1066)     å…±( 33.90%):   å…±( 33.90%)\n","#87(1062)     å…¬( 99.84%):   å…¬( 99.84%)\n","#88(1385)     å¸(100.00%):   å¸(100.00%)\n","#89(6206)     è¦(100.00%):   è¦(100.00%)\n","#90(3121)     æ”¹(100.00%):   æ”¹(100.00%)\n","#91(1587)     å–„(100.00%):   å–„(100.00%)\n","#92(8024)     ï¼Œ( 99.87%):   ï¼Œ( 99.87%)\n","#93(4212)     ç…§( 94.10%):   ç…§( 94.10%)\n","#94(4275)     ç‰‡( 97.66%):   ç‰‡( 97.66%)\n","#95(3625)     æ­Œ( 99.88%):   æ­Œ( 99.88%)\n","#96(2797)     æ‰‹( 99.68%):   æ‰‹( 99.68%)\n","#97(2128)     å®‰(100.00%):   å®‰(100.00%)\n","#98(1059)     å…¨(100.00%):   å…¨(100.00%)\n","#99(511)     ã€‚( 99.99%):   ã€‚( 99.99%)\n"]}],"source":["for sentence, char_tokens, char_porbs, top1_tokens, top1_probs in zip(\n","        sentences,\n","        outputs[\"char_token\"], outputs[\"char_prob\"], outputs[\"top1_token\"],\n","        outputs[\"top1_prob\"]):\n","    tokens = list(char_tokens.numpy())\n","    tokens = tokens[:tokens.index(0)] #crop padding\n","    words_info = sentenceInfoParser.get_words_info(sentence, tokens)\n","    for token_idx, char_token in enumerate(tokens):\n","        if char_token == 101:\n","            continue\n","        if char_token == 102:\n","            break\n","        else:\n","            word_start_idx, word_end_idx = words_info[token_idx]\n","            word = sentence[word_start_idx:word_end_idx]\n","            char = vocab[char_token]\n","            char_prob = char_porbs[token_idx]\n","            top1_token = top1_tokens[token_idx]\n","            top1_char = vocab[top1_token]\n","            top1_prob = top1_probs[token_idx]\n","            print(f\"#{token_idx:0>2}({char_token}) {word:>5}({char_prob:>7.2%}): {top1_char:>3}({top1_prob:>7.2%})\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.9.13 64-bit ('3.9.13')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"42285567d25b4285133252383bb21604aee132a4bd119b875b859f48019e7f4e"}}},"nbformat":4,"nbformat_minor":0}
